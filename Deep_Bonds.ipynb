{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wh267cuhtp",
   "metadata": {},
   "source": [
    "# Bond Yield Prediction using LSTM Encoder-Decoder Architecture\n",
    "\n",
    "This notebook implements a deep learning approach for predicting US bond yields using an LSTM encoder-decoder architecture with professor forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HMe3kSubZ9FY",
   "metadata": {
    "id": "HMe3kSubZ9FY"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Hyperparamter Optimization\n",
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e334c248",
   "metadata": {},
   "source": [
    "# Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y271a_w0ayHT",
   "metadata": {
    "id": "y271a_w0ayHT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Model hyperparameters\n",
    "SEQUENCE_LENGTH = 22\n",
    "INPUT_SIZE = 3\n",
    "HIDDEN_SIZE = 50\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.006\n",
    "N_EPOCHS = 500\n",
    "TARGET_LENGTH = 22\n",
    "\n",
    "# Data split ratios\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.75  # 75% of training data for validation split\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236fea22",
   "metadata": {},
   "source": [
    "# Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f304de-24f2-4b58-a198-a9dd80a0cea7",
   "metadata": {
    "id": "f3f304de-24f2-4b58-a198-a9dd80a0cea7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and perform initial processing of bond and economic data.\"\"\"\n",
    "    \n",
    "    # Load main bond data\n",
    "    bond_data_path = os.getenv('BOND_DATA_PATH', '/content/sample_data/us-government-bond.csv')\n",
    "    df = pd.read_csv(bond_data_path)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Load CPI data\n",
    "    cpi_data_path = os.getenv('CPI_DATA_PATH', '/content/sample_data/CORESTICKM159SFRBATL.csv')\n",
    "    cpi = pd.read_csv(cpi_data_path)\n",
    "    cpi[\"DATE\"] = pd.to_datetime(cpi[\"DATE\"])\n",
    "    cpi = cpi.rename(columns={\"DATE\": \"date\"})\n",
    "    \n",
    "    # Load ISM data  \n",
    "    ism_data_path = os.getenv('ISM_DATA_PATH', '/content/sample_data/AMTMNO.csv')\n",
    "    ism = pd.read_csv(ism_data_path)\n",
    "    ism[\"DATE\"] = pd.to_datetime(ism[\"DATE\"])\n",
    "    ism = ism.rename(columns={\"DATE\": \"date\"})\n",
    "    \n",
    "    return df, cpi, ism\n",
    "\n",
    "# Load the data\n",
    "df, cpi, ism = load_data()\n",
    "print(f\"Loaded bond data shape: {df.shape}\")\n",
    "print(f\"Loaded CPI data shape: {cpi.shape}\")\n",
    "print(f\"Loaded ISM data shape: {ism.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd4268",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2257bc-aad0-4096-b1c2-d18c8ef5582b",
   "metadata": {
    "id": "dd2257bc-aad0-4096-b1c2-d18c8ef5582b"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df, cpi, ism):\n",
    "    \"\"\"Complete data preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    # Process main dataframe\n",
    "    df['date'] = pd.to_datetime(df['date'], format=\"%d/%m/%Y\")\n",
    "    df['DivYield'] = df['DivYield'].replace('%', '', regex=True)\n",
    "    df[\"DivYield\"] = pd.to_numeric(df[\"DivYield\"])\n",
    "    \n",
    "    # Merge datasets\n",
    "    df = pd.merge(df, ism, how=\"left\", on=\"date\")\n",
    "    df = pd.merge(df, cpi, how=\"left\", on=\"date\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.fillna(method=\"backfill\", inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create feature matrix and target variable.\"\"\"\n",
    "    # Separate features and target\n",
    "    features_df = df.drop(['date'], axis=1)\n",
    "    \n",
    "    # Define columns to scale (all except target)\n",
    "    columns_to_scale = [col for col in features_df.columns if col != 'us_5_year_yields']\n",
    "    df_to_scale = features_df[columns_to_scale]\n",
    "    df_unscaled = features_df[['us_5_year_yields']]\n",
    "    \n",
    "    # Apply scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_to_scale)\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale, index=features_df.index)\n",
    "    \n",
    "    # Combine scaled and unscaled data\n",
    "    features_df = pd.concat([scaled_df, df_unscaled], axis=1)\n",
    "    \n",
    "    return features_df, scaler\n",
    "\n",
    "# Preprocess data\n",
    "df = preprocess_data(df, cpi, ism)\n",
    "features_df, scaler = create_features(df)\n",
    "\n",
    "print(f\"Preprocessed data shape: {features_df.shape}\")\n",
    "print(f\"Features: {list(features_df.columns)}\")\n",
    "features_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jswWOfBX7qMd",
   "metadata": {
    "id": "jswWOfBX7qMd"
   },
   "outputs": [],
   "source": [
    "#features_df = features_df.drop(columns=[\"deficit_as_percent_of_gdp\", \"daily_us_real_gdp\",\"DivYield\", \"us_10_year_yields\",\"us_1_year_yields\"])\n",
    "# Using 80% of data as training data\n",
    "train_size = int(len(features_df) * .8)\n",
    "\n",
    "# Train test splitting\n",
    "train_df, test_df = features_df[:train_size], features_df[train_size + 1:]\n",
    "train_df.shape, test_df.shape\n",
    "y_train = train_df['us_5_year_yields']\n",
    "x_train = train_df.drop('us_5_year_yields', axis=1)\n",
    "x_test = test_df.drop('us_5_year_yields', axis=1)\n",
    "y_test = test_df['us_5_year_yields']\n",
    "## Principal Component Analysis and Data Splitting\n",
    "\n",
    "def perform_pca_and_split(features_df, n_components=3, train_ratio=0.8):\n",
    "    \"\"\"Perform PCA analysis and train-test split.\"\"\"\n",
    "    # Split data\n",
    "    train_size = int(len(features_df) * train_ratio)\n",
    "    train_df, test_df = features_df[:train_size], features_df[train_size + 1:]\n",
    "    \n",
    "    # Separate features and target\n",
    "    y_train = train_df['us_5_year_yields']\n",
    "    x_train = train_df.drop('us_5_year_yields', axis=1)\n",
    "    x_test = test_df.drop('us_5_year_yields', axis=1)\n",
    "    y_test = test_df['us_5_year_yields']\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    cov_matrix = np.cov(x_train, rowvar=False)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    x_train_pca = pca.fit_transform(x_train)\n",
    "    x_test_pca = pca.transform(x_test)  # Use transform, not fit_transform for test\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    x_train_pca = pd.DataFrame(x_train_pca, columns=[f'Component {i+1}' for i in range(n_components)])\n",
    "    x_test_pca = pd.DataFrame(x_test_pca, columns=[f'Component {i+1}' for i in range(n_components)])\n",
    "    \n",
    "    return x_train_pca, x_test_pca, y_train, y_test, pca, cov_matrix\n",
    "\n",
    "# Perform PCA and data splitting\n",
    "x_train, x_test, y_train, y_test, pca, cov_matrix = perform_pca_and_split(features_df)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71203bb3",
   "metadata": {},
   "source": [
    "# Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5BTL0iQ252Jm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "5BTL0iQ252Jm",
    "outputId": "759f7aa2-f964-4b0a-ea2c-dddd398768e1"
   },
   "outputs": [],
   "source": [
    "## Data Visualization Functions\n",
    "\n",
    "def plot_covariance_matrix(cov_matrix):\n",
    "    \"\"\"Plot covariance matrix heatmap.\"\"\"\n",
    "    cov_matrix_df = pd.DataFrame(cov_matrix)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=cov_matrix_df.values,\n",
    "        x=list(range(cov_matrix_df.shape[1])),\n",
    "        y=list(range(cov_matrix_df.shape[0])),\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Covariance'),\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Feature Covariance Matrix Heatmap',\n",
    "        xaxis_title='Feature Index',\n",
    "        yaxis_title='Feature Index',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_pca_explained_variance(pca):\n",
    "    \"\"\"Plot PCA explained variance.\"\"\"\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Bar plot for explained variance\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[f'PC{i+1}' for i in range(len(explained_variance))],\n",
    "        y=explained_variance,\n",
    "        name='Explained Variance',\n",
    "        yaxis='y'\n",
    "    ))\n",
    "    \n",
    "    # Line plot for cumulative explained variance\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[f'PC{i+1}' for i in range(len(cumulative_variance))],\n",
    "        y=cumulative_variance,\n",
    "        mode='lines+markers',\n",
    "        name='Cumulative Explained Variance',\n",
    "        yaxis='y2'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='PCA Explained Variance Analysis',\n",
    "        xaxis_title='Principal Components',\n",
    "        yaxis=dict(title='Explained Variance', side='left'),\n",
    "        yaxis2=dict(title='Cumulative Variance', side='right', overlaying='y'),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_feature_loadings(pca, feature_names):\n",
    "    \"\"\"Plot feature loadings for first principal component.\"\"\"\n",
    "    loadings = pca.components_[0]\n",
    "    loadings_df = pd.DataFrame(loadings, index=feature_names, columns=['Loading'])\n",
    "    \n",
    "    # Generate colors\n",
    "    colors = [f'rgb({np.random.randint(0, 255)}, {np.random.randint(0, 255)}, {np.random.randint(0, 255)})' \n",
    "              for _ in feature_names]\n",
    "    \n",
    "    fig = go.Figure(data=go.Bar(\n",
    "        x=loadings_df.index,\n",
    "        y=loadings_df['Loading'],\n",
    "        marker_color=colors,\n",
    "        name='Feature Loadings'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Feature Loadings for First Principal Component',\n",
    "        xaxis_title='Features',\n",
    "        yaxis_title='Loading Weight',\n",
    "        template='plotly_white',\n",
    "        xaxis_tickangle=-45\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create visualizations\n",
    "cov_fig = plot_covariance_matrix(cov_matrix)\n",
    "pca_fig = plot_pca_explained_variance(pca)\n",
    "\n",
    "# Get feature names for loadings plot\n",
    "feature_names = [col for col in features_df.columns if col != 'us_5_year_yields']\n",
    "loadings_fig = plot_feature_loadings(pca, feature_names)\n",
    "\n",
    "print(\"Covariance matrix shape:\", cov_matrix.shape)\n",
    "print(\"PCA loadings shape:\", pca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ZMwehFB1kvB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "0ZMwehFB1kvB",
    "outputId": "6bd1d656-8a33-4da7-fb17-64400ebda557"
   },
   "outputs": [],
   "source": [
    "# Create the bar plot for explained variance\n",
    "bar_plot = go.Bar(\n",
    "    x=[f'PC{i+1}' for i in range(len(explained_variance))],\n",
    "    y=explained_variance,\n",
    "    name='Explained Variance'\n",
    ")\n",
    "\n",
    "# Create the line plot for cumulative explained variance\n",
    "line_plot = go.Scatter(\n",
    "    x=[f'PC{i+1}' for i in range(len(cumulative_variance))],\n",
    "    y=cumulative_variance,\n",
    "    mode='lines+markers',\n",
    "    name='Cumulative Explained Variance'\n",
    ")\n",
    "\n",
    "# Combine both plots\n",
    "fig = go.Figure(data=[bar_plot, line_plot])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Explained Variance by PCA Components',\n",
    "    xaxis_title='Principal Components',\n",
    "    yaxis_title='Variance Explained',\n",
    "    yaxis=dict(range=[0, 1]),  # Ensuring the y-axis goes from 0 to 1\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ZqVWUCR_qT_",
   "metadata": {
    "id": "8ZqVWUCR_qT_"
   },
   "outputs": [],
   "source": [
    "def random_color():\n",
    "    return f'rgb({random.randint(0, 255)}, {random.randint(0, 255)}, {random.randint(0, 255)})'\n",
    "colors = [random_color() for _ in feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dc642",
   "metadata": {},
   "source": [
    "# Preparing data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a68f4-fd1a-4599-bcbe-b29704bc9363",
   "metadata": {
    "id": "995a68f4-fd1a-4599-bcbe-b29704bc9363"
   },
   "outputs": [],
   "source": [
    "## Data Preparation for LSTM\n",
    "\n",
    "def split_dataframe(df, chunk_size):\n",
    "    \"\"\"Split dataframe into chunks of specified size, filtering out small chunks.\"\"\"\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    filtered_chunks = [chunk for chunk in chunks if len(chunk) >= chunk_size]\n",
    "    return filtered_chunks\n",
    "\n",
    "def prepare_sequences(x_train, x_test, y_train, y_test, sequence_length=SEQUENCE_LENGTH):\n",
    "    \"\"\"Convert data into sequences for LSTM training.\"\"\"\n",
    "    \n",
    "    # Split into sequences\n",
    "    x_train_seq = split_dataframe(x_train, sequence_length)\n",
    "    x_test_seq = split_dataframe(x_test, sequence_length)\n",
    "    y_train_seq = split_dataframe(y_train, sequence_length)\n",
    "    y_test_seq = split_dataframe(y_test, sequence_length)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    x_train_np = [np.array(seq) for seq in x_train_seq]\n",
    "    x_test_np = [np.array(seq) for seq in x_test_seq]\n",
    "    y_train_np = [np.array(seq) for seq in y_train_seq]\n",
    "    y_test_np = [np.array(seq) for seq in y_test_seq]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(x_train_np).type(torch.float32)\n",
    "    Y_train = torch.tensor(y_train_np).type(torch.float32)\n",
    "    X_test = torch.tensor(x_test_np).type(torch.float32)\n",
    "    Y_test = torch.tensor(y_test_np).type(torch.float32)\n",
    "    \n",
    "    # Transpose for LSTM input format: (seq_len, batch_size, input_size)\n",
    "    X_train = X_train.transpose(0, 1)\n",
    "    Y_train = Y_train.transpose(0, 1).reshape(sequence_length, -1, 1)\n",
    "    X_test = X_test.transpose(0, 1)\n",
    "    Y_test = Y_test.transpose(0, 1).reshape(sequence_length, -1, 1)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "# Prepare sequences\n",
    "X_train, Y_train, X_test, Y_test = prepare_sequences(x_train, x_test, y_train, y_test)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")\n",
    "# Move to device\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513fe03",
   "metadata": {},
   "source": [
    "# Encoder and Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DF0ESvbJiBsc",
   "metadata": {
    "id": "DF0ESvbJiBsc"
   },
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\"LSTM Encoder for sequence-to-sequence prediction.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \"\"\"Forward pass through encoder.\"\"\"\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            x_input.view(x_input.shape[0], x_input.shape[1], self.input_size)\n",
    "        )\n",
    "        return lstm_out, self.hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden states.\"\"\"\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        )\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    \"\"\"LSTM Decoder for sequence-to-sequence prediction.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \"\"\"Forward pass through decoder.\"\"\"\n",
    "        lstm_out, self.hidden = self.lstm(x_input, encoder_hidden_states)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output, self.hidden\n",
    "\n",
    "\n",
    "class LSTMDecoder2(nn.Module):\n",
    "    \"\"\"Alternative LSTM Decoder with different input size.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(LSTMDecoder2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \"\"\"Forward pass through alternative decoder.\"\"\"\n",
    "        lstm_out, self.hidden = self.lstm(x_input, encoder_hidden_states)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output, self.hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f523e",
   "metadata": {},
   "source": [
    "# Discriminator for Professor Forcing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89691e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator for Professor Forcing training.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, linear_size, lin_dropout):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=2, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.linears = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, linear_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(lin_dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(lin_dropout),\n",
    "            nn.Linear(linear_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"Forward pass through discriminator.\"\"\"\n",
    "        batch_size = hidden_states.size(0)\n",
    "        initial_hidden = self.init_hidden(batch_size)\n",
    "        _, rnn_final_hidden = self.lstm(hidden_states, initial_hidden)\n",
    "        \n",
    "        rnn_final_hidden = (\n",
    "            rnn_final_hidden[0].view(batch_size, -1), \n",
    "            rnn_final_hidden[1].view(batch_size, -1)\n",
    "        )\n",
    "        \n",
    "        scores = self.linears(rnn_final_hidden[0])\n",
    "        return scores\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden states for discriminator.\"\"\"\n",
    "        hidden_1 = torch.zeros(2, batch_size, self.hidden_size)\n",
    "        hidden_2 = torch.zeros(2, batch_size, self.hidden_size)\n",
    "        return (hidden_1, hidden_2)\n",
    "\n",
    "# Initialize loss functions\n",
    "criterion = nn.MSELoss()\n",
    "binary_cross_entropy = nn.BCELoss()\n",
    "\n",
    "print(\"Model architecture defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b8b68",
   "metadata": {},
   "source": [
    "# Main LSTM Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0NhXXwL1mGI",
   "metadata": {
    "id": "a0NhXXwL1mGI"
   },
   "outputs": [],
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    \"\"\"Complete LSTM Encoder-Decoder model with training and prediction capabilities.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = LSTMEncoder(input_size=input_size, hidden_size=hidden_size, num_layers=2)\n",
    "        self.decoder = LSTMDecoder(input_size=input_size, hidden_size=hidden_size, num_layers=2)\n",
    "        self.decoder2 = LSTMDecoder2(input_size=1, hidden_size=hidden_size, num_layers=2)\n",
    "\n",
    "    def train_model(self, input_tensor, target_tensor, n_epochs, target_len, batch_size, \n",
    "                   training_prediction=\"recursive\", teacher_forcing_ratio=0.5, \n",
    "                   learning_rate=0.01, dynamic_tf=False):\n",
    "        \"\"\"Train the model with specified parameters.\"\"\"\n",
    "        losses = np.full(n_epochs, np.nan)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        n_batches = int(input_tensor.shape[1] // batch_size)\n",
    "        \n",
    "        print(f\"Training with {n_batches} batches\")\n",
    "        \n",
    "        with trange(n_epochs) as tr:\n",
    "            for it in tr:\n",
    "                batch_loss = 0\n",
    "                \n",
    "                for b in range(n_batches):\n",
    "                    # Get batch data\n",
    "                    input_batch = input_tensor[:, b: b + batch_size, :]\n",
    "                    target_batch = target_tensor[:, b: b + batch_size, :]\n",
    "                    outputs = torch.zeros(target_len, batch_size, 1)\n",
    "                    \n",
    "                    # Initialize encoder\n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_size=batch_size)\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Encode\n",
    "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
    "                    decoder_input = input_batch[-1, :, :]\n",
    "                    \n",
    "                    # Prepare decoder hidden state\n",
    "                    hidden_state = encoder_hidden[0]\n",
    "                    cell_state = encoder_hidden[1]\n",
    "                    decoder_hidden = (hidden_state[:, 0, :], cell_state[:, 0, :])\n",
    "                    \n",
    "                    # Decode based on training strategy\n",
    "                    if training_prediction == \"recursive\":\n",
    "                        for t in range(target_len):\n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = decoder_output\n",
    "                            \n",
    "                    elif training_prediction == \"teacher_forcing\":\n",
    "                        for t in range(target_len):\n",
    "                            if t == 0:\n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "                            else:\n",
    "                                decoder_output, decoder_hidden = self.decoder2(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "                    \n",
    "                    # Calculate loss and backpropagate\n",
    "                    target_batch = target_batch.reshape(target_batch.shape[0], target_batch.shape[1], 1)\n",
    "                    loss = criterion(outputs, target_batch)\n",
    "                    batch_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                losses[it] = batch_loss\n",
    "                if dynamic_tf and teacher_forcing_ratio > 0:\n",
    "                    teacher_forcing_ratio = teacher_forcing_ratio - 0.02\n",
    "                    \n",
    "                tr.set_postfix(loss=f\"{batch_loss:.3f}\")\n",
    "        \n",
    "        # Save model\n",
    "        model_save_path = os.getenv('MODEL_SAVE_PATH', 'trained_model.pth')\n",
    "        torch.save(self.state_dict(), model_save_path)\n",
    "        return sum(losses) / len(losses)\n",
    "\n",
    "    def predict(self, input_tensor, target_len):\n",
    "        \"\"\"Generate predictions using the trained model.\"\"\"\n",
    "        encoder_output, encoder_hidden = self.encoder(input_tensor)\n",
    "        outputs = torch.zeros(target_len, input_tensor.shape[1], 1)\n",
    "        # Prepare decoder\n",
    "        decoder_input = input_tensor[-1, :, :]\n",
    "        hidden_state = encoder_hidden[0]\n",
    "        cell_state = encoder_hidden[1]\n",
    "        decoder_hidden = (hidden_state[:, 0, :], cell_state[:, 0, :])\n",
    "        \n",
    "        # Generate predictions\n",
    "        for t in range(target_len):\n",
    "            if t == 0:\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[t] = decoder_output\n",
    "                decoder_input = decoder_output\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = self.decoder2(decoder_input, decoder_hidden)\n",
    "                outputs[t] = decoder_output\n",
    "                decoder_input = decoder_output\n",
    "        \n",
    "        return outputs.detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ef166",
   "metadata": {},
   "source": [
    "# Alternate Sequence to Sequence Model with Professor Forcing training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelAlternate(LSTMSeq2Seq):\n",
    "    \"\"\"Alternative model with Professor Forcing training.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ModelAlternate, self).__init__(hidden_size=hidden_size, input_size=input_size)\n",
    "        self.discriminator = Discriminator(\n",
    "            input_size=1, \n",
    "            hidden_size=hidden_size, \n",
    "            linear_size=64, \n",
    "            lin_dropout=0.5\n",
    "        )\n",
    "        self.other_params = [\n",
    "            {'params': self.encoder.parameters(), 'lr': 0.0001},\n",
    "            {'params': self.decoder.parameters(), 'lr': 0.0002},\n",
    "            {'params': self.decoder2.parameters(), 'lr': 0.0003, 'weight_decay': 1e-4}\n",
    "        ]\n",
    "\n",
    "    def adversarial_train(self, learning_rate, input_tensor, target_tensor, \n",
    "                         n_epochs, target_len, batch_size):\n",
    "        \"\"\"Train model using adversarial approach with discriminator.\"\"\"\n",
    "        losses = np.full(n_epochs, np.nan)\n",
    "        gen_optimizer = optim.SGD(self.other_params)\n",
    "        disc_optimizer = optim.SGD(self.discriminator.parameters(), lr=0.003)\n",
    "        \n",
    "        n_batches = int(input_tensor.shape[1] // batch_size)\n",
    "        \n",
    "        with trange(n_epochs) as tr:\n",
    "            for it in tr:\n",
    "                for b in range(n_batches):\n",
    "                    input_batch = input_tensor[:, b:b + batch_size, :]\n",
    "                    target_batch = target_tensor[:, b:b + batch_size, :]\n",
    "                    outputs = torch.zeros(target_len, batch_size, 1).to(input_tensor.device)\n",
    "                    labels = torch.zeros(target_len, batch_size, 1).to(input_tensor.device)\n",
    "                    \n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_size=batch_size)\n",
    "                    gen_optimizer.zero_grad()\n",
    "                    disc_optimizer.zero_grad()\n",
    "                    \n",
    "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
    "                    decoder_input = input_batch[-1, :, :]\n",
    "                    \n",
    "                    hidden_state = encoder_hidden[0]\n",
    "                    cell_state = encoder_hidden[1]\n",
    "                    decoder_hidden = (hidden_state[:, 0, :], cell_state[:, 0, :])\n",
    "                    \n",
    "                    for t in range(target_len):\n",
    "                        if t == 0:\n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = target_batch[t, :, :]\n",
    "                        else:\n",
    "                            decoder_output, decoder_hidden = self.decoder2(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = torch.cat([\n",
    "                                decoder_output[0:25, :], \n",
    "                                target_batch[t, 0:25, :]\n",
    "                            ], dim=0)\n",
    "                        \n",
    "                        labels[t] = torch.cat([torch.ones(25, 1), torch.zeros(25, 1)], dim=0)\n",
    "                    \n",
    "                    labels = labels.transpose(1, 0)\n",
    "                    outputs = outputs.transpose(1, 0)\n",
    "                    \n",
    "                    preds = self.discriminator(outputs)\n",
    "                    indices = torch.randperm(preds.size(0))\n",
    "                    preds = preds[indices]\n",
    "                    labels = labels[indices][:, :, 0, :]\n",
    "                    \n",
    "                    discriminator_loss = binary_cross_entropy(preds, labels)\n",
    "                    generator_loss = -discriminator_loss\n",
    "                    \n",
    "                    if b % 2 == 0:\n",
    "                        generator_loss.backward()\n",
    "                        gen_optimizer.step()\n",
    "                    else:\n",
    "                        discriminator_loss.backward()\n",
    "                        disc_optimizer.step()\n",
    "        \n",
    "        # Save model\n",
    "        model_save_path = os.getenv('MODEL_SAVE_PATH', 'trained_model.pth')\n",
    "        torch.save(self.state_dict(), model_save_path)\n",
    "\n",
    "print(\"Main model classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tnznqZX6y940",
   "metadata": {
    "id": "tnznqZX6y940"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193a2910",
   "metadata": {},
   "source": [
    "# Helper Functions for Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SILp8a9OTHrP",
   "metadata": {
    "id": "SILp8a9OTHrP"
   },
   "outputs": [],
   "source": [
    "def list_to_numpy(list_of_lists):\n",
    "    \"\"\"\n",
    "    Convert all lists in a list of lists to NumPy arrays.\n",
    "    \"\"\"\n",
    "    collection =  [np.array(sublist) for sublist in list_of_lists]\n",
    "    return collection\n",
    "\n",
    "def numpy_to_torch(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    '''\n",
    "    Convert numpy array to PyTorch tensor\n",
    "    '''\n",
    "    X_train_torch = torch.tensor(Xtrain).type(torch.float32)\n",
    "    Y_train_torch = torch.tensor(Ytrain).type(torch.float32)\n",
    "\n",
    "    X_test_torch = torch.tensor(Xtest).type(torch.float32)\n",
    "    Y_test_torch = torch.tensor(Ytest).type(torch.float32)\n",
    "\n",
    "    return X_train_torch, Y_train_torch, X_test_torch, Y_test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J70WxvIfUGSB",
   "metadata": {
    "id": "J70WxvIfUGSB"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train = list_to_numpy(x_train)\n",
    "x_test = list_to_numpy(x_test)\n",
    "y_train = list_to_numpy(y_train)\n",
    "y_test = list_to_numpy(y_test)\n",
    "X_train, Y_train, X_test, Y_test = numpy_to_torch(x_train, y_train, x_test, y_test)\n",
    "X_train = X_train.transpose(0, 1)  # From (batch_size, seq_len, input_length) to (seq_len, batch_size, input_length)\n",
    "Y_train = Y_train.transpose(0, 1)  # From (seq_len, batch_size, 1) to (batch_size, seq_len, 1)\n",
    "X_test = X_test.transpose(0, 1)    # From (batch_size, seq_len, input_length) to (seq_len, batch_size, input_length)\n",
    "Y_test = Y_test.transpose(0, 1)\n",
    "Y_test = Y_test.reshape(22,50,1)\n",
    "Y_train = Y_train.reshape(22,200,1)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf2e55",
   "metadata": {},
   "source": [
    "# Main Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3870c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_seq2seq(input_size = 3, hidden_size = 50)\n",
    "model = model.to(device)\n",
    "loss = model.train_model(X_train, Y_train, n_epochs = 500, target_len = 22, batch_size = 50, training_prediction = 'teacher_forcing', teacher_forcing_ratio = 0.6, learning_rate = 0.006, dynamic_tf = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dp1y53wyTKlo",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "dp1y53wyTKlo",
    "outputId": "b76550cf-6f33-47c9-99e1-c3e6b893d20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:38<00:00,  3.16it/s, loss=0.025]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f9f11",
   "metadata": {},
   "source": [
    "# Training with Professor Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v4IaenrPE6wT",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "v4IaenrPE6wT"
   },
   "outputs": [],
   "source": [
    "extra = model_alternate(input_size = 3, hidden_size = 50).to(device)\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "extra.adversarial_train(input_tensor=X_train, target_tensor=Y_train, n_epochs = 500, target_len = 22, batch_size = 50, learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e1fff",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ueSSGtwNZxAL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "collapsed": true,
    "id": "ueSSGtwNZxAL",
    "outputId": "769bd40b-20e7-4e3e-9e12-ea1fdd932b5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "param_bounds = {\n",
    "    'epochs': (400, 500),            \n",
    "    'learning_rate': (0.0001, 0.01), \n",
    "    'batch_size': (16, 128),         \n",
    "    'hidden_size': (50, 300)         \n",
    "}\n",
    "\n",
    "def objective_function(params):\n",
    "    epochs = int(params[0])\n",
    "    learning_rate = params[1]\n",
    "    batch_size = int(params[2])\n",
    "    hidden_size = int(params[3])\n",
    "    model_2 = lstm_seq2seq(input_size=3, hidden_size= hidden_size)\n",
    "    losses = model_2.train_model(X_train_split, Y_train_split, n_epochs=epochs, target_len=22,\n",
    "                                 batch_size=batch_size, training_prediction='teacher_forcing',\n",
    "                                 teacher_forcing_ratio=0.6, learning_rate=learning_rate, dynamic_tf=False)\n",
    "\n",
    "    Y_val_pred = model_2.predict(X_val_split, 22)\n",
    "    mse = calculate_mse(Y_val_pred, Y_val_split)\n",
    "    return mse[0]\n",
    "\n",
    "num_iterations = 1  \n",
    "popsize = 1  \n",
    "total_evaluations = num_iterations * popsize\n",
    "result = differential_evolution(objective_function,\n",
    "                                    bounds=[param_bounds['epochs'], param_bounds['learning_rate'],\n",
    "                                            param_bounds['batch_size'], param_bounds['hidden_size']],\n",
    "                                    strategy='best1bin', maxiter=num_iterations, popsize=popsize,\n",
    "                                    tol=0.01, seed=42, disp=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf-2.0]",
   "language": "python",
   "name": "conda-env-tf-2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
